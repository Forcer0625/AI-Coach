import sys
import cv2
import os
import pyaudio
import wave
import speech_recognition as sr
from faster_whisper import WhisperModel
import openai
from PyQt6.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLabel, QTextEdit, QHBoxLayout
from PyQt6.QtGui import QImage, QPixmap
from PyQt6.QtCore import QTimer, Qt, QThread, pyqtSignal
import numpy as np

os.environ["PATH"] += ";C:\\Program Files\\NVIDIA\\CUDNN\\v9.7\\bin\\12.8"
os.environ["LD_LIBRARY_PATH"] = "C:\\Program Files\\NVIDIA\\CUDNN\\v9.7\\bin\\12.8"

# OpenAI API Key (è«‹æ›¿æ›ç‚ºä½ çš„ API Key)
OPENAI_API_KEY = "your_openai_api_key"
openai.api_key = OPENAI_API_KEY

# è¨­å®šä½¿ç”¨çš„ GPT æ¨¡å‹åç¨±ï¼ˆGPT-4 or GPT-4V for visionï¼‰
GPT_MODEL_NAME = "gpt-4-vision-preview"

# åˆå§‹åŒ– Whisper æœ¬åœ°æ¨¡å‹
whisper_model = WhisperModel("medium", device="cuda", compute_type="float16")

class CameraThread(QThread):
    frame_update = pyqtSignal(QImage)
    
    def __init__(self):
        super().__init__()
        self.cap = cv2.VideoCapture(0)
        self.running = True
    
    def run(self):
        while self.running:
            ret, frame = self.cap.read()
            if ret:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                height, width, channel = frame.shape
                qimg = QImage(frame.data, width, height, width * channel, QImage.Format.Format_RGB888)
                self.frame_update.emit(qimg)
            cv2.waitKey(1)
    
    def stop(self):
        self.running = False
        self.quit()
        self.wait()
        self.cap.release()

class VoiceProcessingThread(QThread):
    transcription_done = pyqtSignal(str)
    image_captured = pyqtSignal(str)
    
    def __init__(self, camera_thread, captured_frame_count):
        super().__init__()
        self.camera_thread = camera_thread
        self.running = True
        self.count = captured_frame_count
    
    def run(self):
        self.process_voice_input()
    
    def process_voice_input(self):
        audio_file = "./temp/voice_input.wav"
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            #print("é–‹å§‹éŒ„éŸ³...")
            recognizer.adjust_for_ambient_noise(source, duration=1)
            recognizer.energy_threshold = 300
            try:
                audio_data = recognizer.listen(source, timeout=5, phrase_time_limit=5)
                with open(audio_file, "wb") as f:
                    f.write(audio_data.get_wav_data())
                #print("éŒ„éŸ³çµæŸ")
            except sr.WaitTimeoutError:
                #print("âŒ éŒ„éŸ³è¶…æ™‚ï¼Œæœªåµæ¸¬åˆ°èªéŸ³")
                self.transcription_done.emit("âŒ éŒ„éŸ³è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡")
                return
        
        # æ“·å–ç•¶å‰æ”å½±æ©Ÿå½±åƒ
        ret, frame = self.camera_thread.cap.read()
        if ret:
            img_path = "./temp/captured_frame"+str(self.count)+".jpg"
            cv2.imwrite(img_path, frame)
            self.image_captured.emit(img_path)
        
        # èªéŸ³è½‰æ–‡å­—
        segments, _ = whisper_model.transcribe(audio_file)
        transcribed_text = " ".join([segment.text for segment in segments])
        self.transcription_done.emit(transcribed_text)

class AICoachApp(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        self.initCamera()
        self.img_path = None
        self.captured_frame_count = 0
        self.chat_history = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åå°ˆæ¥­çš„ AI å¥èº«æ•™ç·´ï¼Œæ“…é•·æ ¹æ“šä½¿ç”¨è€…çš„å‹•ä½œã€é«”æ…‹å’ŒèªéŸ³å•é¡Œæä¾›å°ˆæ¥­çš„å¥èº«å»ºè­°ï¼Œè«‹å°ˆæ³¨æ–¼å¥èº«é ˜åŸŸçš„çŸ¥è­˜ï¼Œä¸¦æä¾›æ­£ç¢ºçš„é‹å‹•æŒ‡å°ã€‚è«‹è‡ªè¡Œåˆ¤æ–·æ˜¯å¦ä½¿ç”¨åœ–åƒç”Ÿæˆå·¥å…·å¹«åŠ©ä½¿ç”¨è€…ï¼Œå¦‚æœéœ€è¦ï¼Œè«‹åœ¨å›æ‡‰çš„çµå°¾åŠ ä¸Šé—œéµå­—\"è«‹åƒè€ƒä¸‹é¢çš„åœ–ç‰‡èªªæ˜\", ç„¶å¾Œç”Ÿæˆä¸€æ®µçµ¦DALL-Eçš„promptã€‚æ ¼å¼åƒè€ƒ(å‡è¨­ä½¿ç”¨è€…è©¢å•å¦‚ä½•åšæ·±è¹²):\"1. ä¸€é–‹å§‹åŸºæœ¬çš„å¾’æ‰‹æ·±è¹²è¦å…ˆæŠŠé›™è…³æ‰“é–‹èˆ‡è‚©è†€åŒå¯¬ï¼Œè…³å°–å‘å‰ï¼Œé›™æ‰‹å‰‡å¯æ”¾åœ¨èƒ¸å‰äº¤å‰äº¤ç–Šæˆ–æ˜¯é›™æ‰‹æ¡æ‹³ã€‚\n2. æŠŠè…³åº•å¹³æ”¾åœ¨åœ°ä¸Šï¼Œå°‡é‡å¿ƒæ”¾åœ¨é›™è…³ä¸Šã€‚\n3. å¸æ°£æ™‚å°‡é‡å¿ƒæ…¢æ…¢å¾€å¾Œï¼ŒæŠŠè‡€éƒ¨ç·©ç·©åœ°ä¸‹å¾Œç§»ï¼Œæƒ³åƒå¾Œæ–¹æœ‰ä¸€å¼µæ¤…å­ï¼Œç¶­æŒå€‹3-5ç§’çš„æ™‚é–“ï¼Œå‘¼æ°£å¾Œå†æ…¢æ…¢åœ°å›åˆ°åŸä¾†çš„å‹•ä½œã€‚\nè«‹åƒè€ƒä¸‹é¢çš„åœ–ç‰‡èªªæ˜\nprompt for DALL-E\""}
        ]
    
    def initUI(self):
        self.setWindowTitle("AI å¥èº«æ•™ç·´")
        self.setGeometry(100, 100, 1200, 600)
        
        # å·¦å´ - å°è©±ç´€éŒ„èˆ‡è¼¸å…¥
        self.conversation = QTextEdit(self)
        self.conversation.setReadOnly(True)
        
        self.text_input = QTextEdit(self)
        self.text_input.setPlaceholderText("è¼¸å…¥ä½ çš„å•é¡Œ...")
        
        self.send_button = QPushButton("ç™¼é€", self)
        self.send_button.clicked.connect(self.process_text_input)
        
        self.voice_button = QPushButton("ğŸ¤ èªéŸ³è¼¸å…¥", self)
        self.voice_button.clicked.connect(self.start_voice_processing)
        
        left_layout = QVBoxLayout()
        left_layout.addWidget(QLabel("å°è©±ç´€éŒ„:"))
        left_layout.addWidget(self.conversation)
        left_layout.addWidget(QLabel("è¼¸å…¥:"))
        left_layout.addWidget(self.text_input)
        left_layout.addWidget(self.send_button)
        left_layout.addWidget(self.voice_button)
        
        # å³å´ - æ”å½±æ©Ÿç•«é¢
        self.camera_label = QLabel(self)
        self.camera_label.setFixedSize(500, 400)
        
        right_layout = QVBoxLayout()
        right_layout.addWidget(QLabel("å³æ™‚æ”å½±æ©Ÿç•«é¢:"))
        right_layout.addWidget(self.camera_label)
        
        # ä¸»ä½ˆå±€
        main_layout = QHBoxLayout()
        main_layout.addLayout(left_layout, 2)
        main_layout.addLayout(right_layout, 3)
        
        self.setLayout(main_layout)
        
    def process_text_input(self):
        self.voice_button.setDisabled(True)
        self.send_button.setDisabled(True)
        user_text = self.text_input.toPlainText()
        if user_text.strip():
            self.conversation.append(f'ğŸ§‘â€ğŸ’» ä½ : {user_text}')
            self.text_input.clear()
            self.img_path = None
            self.get_ai_response(user_text, None)
        self.recover_send_button()
        self.recover_voice_button()
    
    def initCamera(self):
        self.camera_thread = CameraThread()
        self.camera_thread.frame_update.connect(self.update_frame)
        self.camera_thread.start()
    
    def update_frame(self, qimg):
        self.camera_label.setPixmap(QPixmap.fromImage(qimg))
    
    def start_voice_processing(self):
        self.voice_button.setDisabled(True)
        self.send_button.setDisabled(True)
        self.voice_button.setText("ğŸ™ï¸ éŒ„éŸ³ä¸­...")
        self.voice_thread = VoiceProcessingThread(self.camera_thread, self.captured_frame_count)
        self.voice_thread.transcription_done.connect(self.display_transcription)
        self.voice_thread.image_captured.connect(self.display_image)
        self.voice_thread.start()
    
    def display_transcription(self, text):
        self.conversation.append(f'ğŸ§‘â€ğŸ’» ä½ : {text}')
        if text == "âŒ éŒ„éŸ³è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡":
            self.recover_voice_button()
            self.recover_send_button()
            return
        else:
            self.captured_frame_count += 1
        self.get_ai_response(text, self.img_path)
        self.recover_voice_button()
        self.recover_send_button()

    def recover_voice_button(self):
        self.voice_button.setDisabled(False)
        self.voice_button.setText("ğŸ¤ èªéŸ³è¼¸å…¥")

    def recover_send_button(self):
        self.send_button.setDisabled(False)
    
    def display_image(self, img_path):
        self.img_path = img_path
        #print(f"å½±åƒæ“·å–å®Œæˆ: {img_path}")
    
    def get_ai_response(self, user_input, image_path=None):
        self.chat_history.append({"role": "user", "content": user_input})

        messages = list(self.chat_history)
        
        if image_path:
            #print("å–å¾—æ“·å–å½±åƒ...")
            messages.append({"role": "user", "content": {"type": "image_url", "image_url": f"file://{image_path}"}})
        
        try:
            response = openai.chat.completions.create(
                model=GPT_MODEL_NAME,
                messages=messages
            )
            ai_text = response["choices"][0]["message"]["content"]
        except:
            ai_text = ""
        finally:
            self.conversation.append(f'ğŸ¤– AI æ•™ç·´: {ai_text}')
            self.chat_history.append({"role": "assistant", "content": ai_text})
            self.img_path = None
            #print(self.chat_history)
    
    def closeEvent(self, event):
        self.camera_thread.stop()
        event.accept()

if __name__ == '__main__':
    app = QApplication(sys.argv)
    window = AICoachApp()
    window.show()
    sys.exit(app.exec())
