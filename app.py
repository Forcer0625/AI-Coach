import sys
import cv2
import os
import pyaudio
import wave
import json
import speech_recognition as sr
from faster_whisper import WhisperModel
import openai
import base64
from PyQt6.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLabel, QTextEdit, QHBoxLayout
from PyQt6.QtGui import QImage, QPixmap
from PyQt6.QtCore import QTimer, Qt, QThread, pyqtSignal
import numpy as np

os.environ["PATH"] += ";C:\\Program Files\\NVIDIA\\CUDNN\\v9.7\\bin\\12.8"
os.environ["LD_LIBRARY_PATH"] = "C:\\Program Files\\NVIDIA\\CUDNN\\v9.7\\bin\\12.8"

# OpenAI API Key (è«‹æ›¿æ›ç‚ºä½ çš„ API Key)
OPENAI_API_KEY = "your_api_key"
openai.api_key = OPENAI_API_KEY

# è¨­å®šä½¿ç”¨çš„ GPT æ¨¡å‹åç¨±ï¼ˆGPT-4 or GPT-4V for visionï¼‰
GPT_MODEL_NAME = "gpt-4o-mini"
DALL_E_MODEL = "dall-e-2"

# åˆå§‹åŒ– Whisper æœ¬åœ°æ¨¡å‹
whisper_model = WhisperModel("medium", device="cuda", compute_type="float16")

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

class CameraThread(QThread):
    frame_update = pyqtSignal(QImage)
    
    def __init__(self):
        super().__init__()
        self.cap = cv2.VideoCapture(0)
        self.running = True
    
    def run(self):
        while self.running:
            ret, frame = self.cap.read()
            if ret:
                frame = frame[80:560]
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                height, width, channel = frame.shape
                qimg = QImage(frame.data, width, height, width * channel, QImage.Format.Format_RGB888)
                self.frame_update.emit(qimg)
            cv2.waitKey(1)
    
    def stop(self):
        self.running = False
        self.quit()
        self.wait()
        self.cap.release()

class VoiceProcessingThread(QThread):
    transcription_done = pyqtSignal(str)
    image_captured = pyqtSignal(str)
    
    def __init__(self, camera_thread, captured_frame_count):
        super().__init__()
        self.camera_thread = camera_thread
        self.running = True
        self.count = captured_frame_count
    
    def run(self):
        self.process_voice_input()
    
    def process_voice_input(self):
        audio_file = "./temp/voice_input"+str(self.count)+".wav"
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            recognizer.adjust_for_ambient_noise(source, duration=1)
            recognizer.energy_threshold = 300
            try:
                audio_data = recognizer.listen(source, timeout=5, phrase_time_limit=10)
                with open(audio_file, "wb") as f:
                    f.write(audio_data.get_wav_data())
            except sr.WaitTimeoutError:
                self.transcription_done.emit("âŒ éŒ„éŸ³è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡")
                return
        
        # æ“·å–ç•¶å‰æ”å½±æ©Ÿå½±åƒ
        ret, frame = self.camera_thread.cap.read()
        if ret:
            img_path = "./temp/captured_frame"+str(self.count)+".jpg"
            compressed_img = frame[80:560] # crop to square
            cv2.imwrite(img_path, compressed_img)
            self.image_captured.emit(img_path)
        
        # èªéŸ³è½‰æ–‡å­—
        segments, _ = whisper_model.transcribe(audio_file)
        transcribed_text = " ".join([segment.text for segment in segments])
        self.transcription_done.emit(transcribed_text)

class AICoachApp(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        self.initCamera()
        self.img_path = None
        self.captured_frame_count = 0
        self.chat_history = [
            {"role": "developer", "content": "ä½ æ˜¯ä¸€åå°ˆæ¥­çš„å¥èº«æ•™ç·´ï¼Œæ“…é•·æ ¹æ“šä½¿ç”¨è€…çš„å‹•ä½œ(æœƒæœ‰åœ–ç‰‡)ã€é«”æ…‹å’Œå¥èº«ç­‰å•é¡Œæä¾›å°ˆæ¥­çš„å¥èº«å»ºè­°ï¼Œè«‹å°ˆæ³¨æ–¼å¥èº«é ˜åŸŸçš„çŸ¥è­˜ï¼Œä¸¦æä¾›æ­£ç¢ºçš„é‹å‹•æŒ‡å°ã€‚ç•¶æœ‰åœ–ç‰‡æ™‚ï¼Œæœƒå”åŠ©ä½¿ç”¨è€…æ ¹æ“šçµ¦äºˆçš„åœ–åƒå›ç­”, è«‹å‹™å¿…åš´æ ¼æŒ‰ç…§åœ–ç‰‡å›è¦†ä½¿ç”¨è€…çš„è«‹æ±‚(é€™æ˜¯æœ€å„ªå…ˆçš„äº‹é …, æ¨¡ç³Šçš„å›ç­”ä¹Ÿå¯ä»¥)"},#è«‹è‡ªè¡Œåˆ¤æ–·æ˜¯å¦ä½¿ç”¨åœ–åƒç”Ÿæˆå·¥å…·å¹«åŠ©ä½¿ç”¨è€…ï¼Œå¦‚æœéœ€è¦ï¼Œè«‹åœ¨å›æ‡‰çš„çµå°¾åŠ ä¸Šé—œéµå­—\"è«‹åƒè€ƒä¸‹é¢çš„åœ–ç‰‡èªªæ˜\", ç„¶å¾Œç”Ÿæˆä¸€æ®µçµ¦DALL-Eçš„promptã€‚æ ¼å¼åƒè€ƒ(å‡è¨­ä½¿ç”¨è€…è©¢å•å¦‚ä½•åšæ·±è¹²):\"1. ä¸€é–‹å§‹åŸºæœ¬çš„å¾’æ‰‹æ·±è¹²è¦å…ˆæŠŠé›™è…³æ‰“é–‹èˆ‡è‚©è†€åŒå¯¬ï¼Œè…³å°–å‘å‰ï¼Œé›™æ‰‹å‰‡å¯æ”¾åœ¨èƒ¸å‰äº¤å‰äº¤ç–Šæˆ–æ˜¯é›™æ‰‹æ¡æ‹³ã€‚\n2. æŠŠè…³åº•å¹³æ”¾åœ¨åœ°ä¸Šï¼Œå°‡é‡å¿ƒæ”¾åœ¨é›™è…³ä¸Šã€‚\n3. å¸æ°£æ™‚å°‡é‡å¿ƒæ…¢æ…¢å¾€å¾Œï¼ŒæŠŠè‡€éƒ¨ç·©ç·©åœ°ä¸‹å¾Œç§»ï¼Œæƒ³åƒå¾Œæ–¹æœ‰ä¸€å¼µæ¤…å­ï¼Œç¶­æŒå€‹3-5ç§’çš„æ™‚é–“ï¼Œå‘¼æ°£å¾Œå†æ…¢æ…¢åœ°å›åˆ°åŸä¾†çš„å‹•ä½œã€‚\nè«‹åƒè€ƒä¸‹é¢çš„åœ–ç‰‡èªªæ˜\nprompt for DALL-E\""}
            # {'role': 'user', 'content': 'å¦‚ä½•æ·±è¹²'},
            # {'role': 'assistant', 'content': "æ·±è¹²æ˜¯ä¸€å€‹å¾ˆå¥½çš„å…¨èº«æ€§è¨“ç·´å‹•ä½œï¼Œèƒ½å¹«åŠ©ä½ å¢å¼·è…¿éƒ¨å’Œæ ¸å¿ƒçš„åŠ›é‡ã€‚ä»¥ä¸‹æ˜¯æ­£ç¢ºçš„æ·±è¹²æ­¥é©Ÿå’Œè¦é»ï¼š\n\n1. **ç«™ä½**ï¼šé›™è…³èˆ‡è‚©åŒå¯¬ï¼Œè…³å°–å¾®å¾®å¤–å±•ï¼Œä¿æŒç©©å®šçš„ç«™å§¿ã€‚\n\n2. **å§¿å‹¢æº–å‚™**ï¼šå°‡èƒ¸éƒ¨æŒºèµ·ï¼Œè‚©è†€æ”¾é¬†ï¼Œçœ¼ç›ç›´è¦–å‰æ–¹ã€‚ç¢ºä¿èƒŒéƒ¨ä¿æŒè‡ªç„¶æ›²ç·šï¼Œé¿å…é§èƒŒã€‚\n\n3. **é–‹å§‹ä¸‹è¹²**ï¼š\n   - åŒæ™‚å½æ›²è†è“‹å’Œé«–é—œç¯€ï¼Œè‡€éƒ¨å‘å¾Œç§»å‹•ï¼Œè®“å±è‚¡åƒååœ¨æ¤…å­ä¸Š ä¸€æ¨£ã€‚\n   - ä¿æŒè†è“‹çš„æ–¹å‘èˆ‡è…³å°–ä¸€è‡´ï¼Œè†è“‹ä¸æ‡‰è¶…éè…³å°–ã€‚\n\n4. **ä¸‹è¹²æ·±åº¦**ï¼šæ ¹æ“šå€‹äººéˆæ´»æ€§ï¼Œå¯ä»¥é¸æ“‡ä¸‹è¹²åˆ°å¤§è…¿èˆ‡åœ°é¢å¹³è¡Œï¼Œæˆ–è€…æ›´ä½ï¼Œè¦–ä½ çš„èˆ’é©åº¦å’Œéˆæ´»æ€§è€Œå®šã€‚\n\n5. **ä¸Šå‡**ï¼šç”¨è…³è·Ÿæ¨åœ°ï¼Œä¿æŒæ ¸å¿ƒæ”¶ç·Šï¼Œç›´ç«‹èµ·ä¾†ï¼Œå›åˆ°èµ·å§‹ä½ç½®ã€‚\n\n6. **å‘¼å¸**ï¼šåœ¨ä¸‹è¹²æ™‚å¸æ°£ï¼Œä¸Šå‡æ™‚å‘¼æ°£ã€‚\n\n### æ³¨æ„äº‹é …ï¼š\n- ç¢ºä¿è†è“‹ä¸å…§æ‰£ï¼Œé€™æ¨£å¯ä»¥æ¸›å°‘å—å‚·çš„é¢¨éšªã€‚\n- å¦‚æœæ„Ÿè¦ºåˆç–²ç´¯æˆ–ä¸ç©©å®šï¼Œè€ƒæ…®æ¸›å°‘è² è·æˆ–ä½¿ç”¨æ”¯æ’ç‰©ã€‚\n- å¦‚æœæœ‰ä»»ä½•ä¸é©æˆ–ç–¼ç—›ï¼Œæ‡‰ç«‹å³åœæ­¢ä¸¦å°‹æ±‚å°ˆæ¥­å»ºè­°ã€‚\n\né€²è¡Œç†±èº«å’Œæ‹‰ä¼¸ä¹Ÿæ˜¯å¾ˆé‡è¦çš„ï¼Œä»¥é˜²æ­¢å—å‚·ä¸¦æé«˜é‹å‹•è¡¨ç¾ã€‚ç¥ä½ è¨“ç·´æ„‰å¿«ï¼"}
            ]
    
    def initUI(self):
        self.setWindowTitle("AI å¥èº«æ•™ç·´")
        self.setGeometry(100, 100, 1200, 600)
        
        # å·¦å´ - å°è©±ç´€éŒ„èˆ‡è¼¸å…¥
        self.conversation = QTextEdit(self)
        self.conversation.setReadOnly(True)
        
        self.text_input = QTextEdit(self)
        self.text_input.setPlaceholderText("è¼¸å…¥ä½ çš„å•é¡Œ...")
        
        self.send_button = QPushButton("ç™¼é€", self)
        self.send_button.clicked.connect(self.process_text_input)
        
        self.voice_button = QPushButton("ğŸ¤ èªéŸ³è¼¸å…¥", self)
        self.voice_button.clicked.connect(self.start_voice_processing)
        
        left_layout = QVBoxLayout()
        left_layout.addWidget(QLabel("å°è©±ç´€éŒ„:"))
        left_layout.addWidget(self.conversation)
        left_layout.addWidget(QLabel("è¼¸å…¥:"))
        left_layout.addWidget(self.text_input)
        left_layout.addWidget(self.send_button)
        left_layout.addWidget(self.voice_button)
        
        # å³å´ - æ”å½±æ©Ÿç•«é¢
        self.camera_label = QLabel(self)
        self.camera_label.setFixedSize(500, 400)
        
        right_layout = QVBoxLayout()
        right_layout.addWidget(QLabel("å³æ™‚æ”å½±æ©Ÿç•«é¢:"))
        right_layout.addWidget(self.camera_label)
        
        # ä¸»ä½ˆå±€
        main_layout = QHBoxLayout()
        main_layout.addLayout(left_layout, 2)
        main_layout.addLayout(right_layout, 3)
        
        self.setLayout(main_layout)
        
    def process_text_input(self):
        self.voice_button.setDisabled(True)
        self.send_button.setDisabled(True)
        user_text = self.text_input.toPlainText()
        if user_text.strip():
            self.conversation.append(f'ğŸ§‘â€ğŸ’» ä½ : {user_text}')
            self.text_input.clear()
            self.img_path = None
            self.get_ai_response(user_text, None)
        self.recover_voice_button()
        self.recover_send_button()
    
    def initCamera(self):
        self.camera_thread = CameraThread()
        self.camera_thread.frame_update.connect(self.update_frame)
        self.camera_thread.start()
    
    def update_frame(self, qimg):
        self.camera_label.setPixmap(QPixmap.fromImage(qimg))
    
    def start_voice_processing(self):
        self.voice_button.setDisabled(True)
        self.send_button.setDisabled(True)
        self.voice_thread = VoiceProcessingThread(self.camera_thread, self.captured_frame_count)
        self.voice_thread.transcription_done.connect(self.display_transcription)
        self.voice_thread.image_captured.connect(self.display_image)
        self.voice_thread.start()
        self.voice_button.setText("ğŸ™ï¸ éŒ„éŸ³ä¸­...")
    
    def display_transcription(self, text):
        self.conversation.append(f'ğŸ§‘â€ğŸ’» ä½ : {text}')
        if text == "âŒ éŒ„éŸ³è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡":
            self.recover_voice_button()
            self.recover_send_button()
            return
        else:
            self.captured_frame_count += 1
        self.get_ai_response(text, self.img_path)
        self.recover_voice_button()
        self.recover_send_button()

    def recover_voice_button(self):
        self.voice_button.setDisabled(False)
        self.voice_button.setText("ğŸ¤ èªéŸ³è¼¸å…¥")

    def recover_send_button(self):
        self.send_button.setDisabled(False)
    
    def display_image(self, img_path):
        self.img_path = img_path
    
    def get_ai_response(self, user_input, image_path=None):
        self.voice_button.setText("ğŸ¤– å›æ‡‰ä¸­...")

        messages = list(self.chat_history)
        
        if image_path:
            base64_image = encode_image(image_path)
            messages.append({"role": "user", "content": [
                        {
                            "type": "text",
                            "text": "(è«‹æ ¹æ“šåœ–ç‰‡å›ç­”)"+user_input
                        },
                        {"type": "image_url", "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }}]
                    })
        else:
            messages.append({"role": "user", "content": user_input})
        
        try:
            response = openai.chat.completions.create(
                model=GPT_MODEL_NAME,
                messages=messages
            )
            ai_text = response.choices[0].message.content
            self.chat_history.append({"role": "user", "content": user_input})
            self.chat_history.append({"role": "assistant", "content": ai_text})
        except:
            ai_text = ""
        finally:
            self.conversation.append(f'ğŸ¤– AI æ•™ç·´: {ai_text}')
            self.img_path = None

        # if "çš„åœ–ç‰‡" in ai_text and "åƒè€ƒ" in ai_text:
        #     ai_text = 0
        #     image_response = openai.Image.create(
        #         #model=DALL_E_MODEL,
        #         prompt=ai_text,
        #         n=1,
        #         size="256x256"
        #     )
        #     image_url = image_response["data"][0]["url"]
        #     self.image_generated.emit(image_url)
    
    def closeEvent(self, event):
        self.camera_thread.stop()
        with open("chat history(only text).json", "w", encoding="utf-8") as fp:
            json.dump(self.chat_history, fp, indent=2, ensure_ascii=False) 
        event.accept()

if __name__ == '__main__':
    app = QApplication(sys.argv)
    window = AICoachApp()
    window.show()
    sys.exit(app.exec())
