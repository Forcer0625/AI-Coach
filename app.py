import sys
import cv2
import os
import pyaudio
import wave
import json
import speech_recognition as sr
from faster_whisper import WhisperModel
import openai
import base64
from PyQt6.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLabel, QTextEdit, QHBoxLayout, QSizePolicy, QFileDialog, QComboBox
from PyQt6.QtGui import QImage, QPixmap
from PyQt6.QtCore import QTimer, Qt, QThread, pyqtSignal
import numpy as np

os.environ["PATH"] += ";C:\\Program Files\\NVIDIA\\CUDNN\\v9.7\\bin\\12.8"
os.environ["LD_LIBRARY_PATH"] = "C:\\Program Files\\NVIDIA\\CUDNN\\v9.7\\bin\\12.8"

# OpenAI API Key (è«‹æ›¿æ›ç‚ºä½ çš„ API Key)
OPENAI_API_KEY = "your_api_key"
openai.api_key = OPENAI_API_KEY

# è¨­å®šä½¿ç”¨çš„ GPT æ¨¡å‹åç¨±ï¼ˆGPT-4 or GPT-4V for visionï¼‰
GPT_MODEL_NAME = "gpt-4o-mini"
DALL_E_MODEL = "dall-e-2"

# åˆå§‹åŒ– Whisper æœ¬åœ°æ¨¡å‹
whisper_model = WhisperModel("large-v3-turbo", device="cuda", compute_type="float16")

INIT_CHAT_HISTORY = [
    {"role": "developer", "content": "ä½ æ˜¯ä¸€åå°ˆæ¥­çš„å¥èº«æ•™ç·´ï¼Œæ“…é•·æ ¹æ“šä½¿ç”¨è€…çš„å‹•ä½œ(æœƒæœ‰åœ–ç‰‡)ã€é«”æ…‹å’Œå¥èº«ç­‰å•é¡Œæä¾›å°ˆæ¥­çš„å¥èº«å»ºè­°ï¼Œè«‹å°ˆæ³¨æ–¼å¥èº«é ˜åŸŸçš„çŸ¥è­˜ï¼Œä¸¦æä¾›æ­£ç¢ºçš„é‹å‹•æŒ‡å°ã€‚ç•¶æœ‰åœ–ç‰‡æ™‚ï¼Œæœƒå”åŠ©ä½¿ç”¨è€…æ ¹æ“šçµ¦äºˆçš„åœ–åƒå›ç­”, è«‹å‹™å¿…åš´æ ¼æŒ‰ç…§åœ–ç‰‡å›è¦†ä½¿ç”¨è€…çš„è«‹æ±‚(é€™æ˜¯æœ€å„ªå…ˆçš„äº‹é …, æ¨¡ç³Šçš„å›ç­”ä¹Ÿå¯ä»¥)"},#è«‹è‡ªè¡Œåˆ¤æ–·æ˜¯å¦ä½¿ç”¨åœ–åƒç”Ÿæˆå·¥å…·å¹«åŠ©ä½¿ç”¨è€…ï¼Œå¦‚æœéœ€è¦ï¼Œè«‹åœ¨å›æ‡‰çš„çµå°¾åŠ ä¸Šé—œéµå­—\"è«‹åƒè€ƒä¸‹é¢çš„åœ–ç‰‡èªªæ˜\", ç„¶å¾Œç”Ÿæˆä¸€æ®µçµ¦DALL-Eçš„promptã€‚æ ¼å¼åƒè€ƒ(å‡è¨­ä½¿ç”¨è€…è©¢å•å¦‚ä½•åšæ·±è¹²):\"1. ä¸€é–‹å§‹åŸºæœ¬çš„å¾’æ‰‹æ·±è¹²è¦å…ˆæŠŠé›™è…³æ‰“é–‹èˆ‡è‚©è†€åŒå¯¬ï¼Œè…³å°–å‘å‰ï¼Œé›™æ‰‹å‰‡å¯æ”¾åœ¨èƒ¸å‰äº¤å‰äº¤ç–Šæˆ–æ˜¯é›™æ‰‹æ¡æ‹³ã€‚\n2. æŠŠè…³åº•å¹³æ”¾åœ¨åœ°ä¸Šï¼Œå°‡é‡å¿ƒæ”¾åœ¨é›™è…³ä¸Šã€‚\n3. å¸æ°£æ™‚å°‡é‡å¿ƒæ…¢æ…¢å¾€å¾Œï¼ŒæŠŠè‡€éƒ¨ç·©ç·©åœ°ä¸‹å¾Œç§»ï¼Œæƒ³åƒå¾Œæ–¹æœ‰ä¸€å¼µæ¤…å­ï¼Œç¶­æŒå€‹3-5ç§’çš„æ™‚é–“ï¼Œå‘¼æ°£å¾Œå†æ…¢æ…¢åœ°å›åˆ°åŸä¾†çš„å‹•ä½œã€‚\nè«‹åƒè€ƒä¸‹é¢çš„åœ–ç‰‡èªªæ˜\nprompt for DALL-E\""}
]

def encode_image(image_path):
    with open(image_path, "rb") as image_file:
        return base64.b64encode(image_file.read()).decode("utf-8")

class CameraThread(QThread):
    frame_update = pyqtSignal(QImage)
    
    def __init__(self):
        super().__init__()
        self.cap = cv2.VideoCapture(0)
        self.running = True
    
    def run(self):
        while self.running:
            ret, frame = self.cap.read()
            if ret:
                frame = frame[:,80:560,:]
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                height, width, channel = frame.shape
                qimg = QImage(frame.data, width, height, width * channel, QImage.Format.Format_RGB888)
                self.frame_update.emit(qimg)
            cv2.waitKey(1)
    
    def stop(self):
        self.running = False
        self.quit()
        self.wait()
        self.cap.release()

class VoiceProcessingThread(QThread):
    transcription_done = pyqtSignal(str)
    image_captured = pyqtSignal(str)
    
    def __init__(self, camera_thread, session_id, count):
        super().__init__()
        self.camera_thread = camera_thread
        self.session_id = session_id
        self.count = count
    
    def run(self):
        self.process_voice_input()
    
    def process_voice_input(self):
        audio_file = "./session/"+self.session_id+"/voice_input"+str(self.count)+".wav"
        recognizer = sr.Recognizer()
        with sr.Microphone() as source:
            recognizer.adjust_for_ambient_noise(source, duration=1)
            recognizer.energy_threshold = 300
            try:
                audio_data = recognizer.listen(source, timeout=5, phrase_time_limit=10)
                with open(audio_file, "wb") as f:
                    f.write(audio_data.get_wav_data())
            except sr.WaitTimeoutError:
                self.transcription_done.emit("âŒ éŒ„éŸ³è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡")
                return
        
        # æ“·å–ç•¶å‰æ”å½±æ©Ÿå½±åƒ
        ret, frame = self.camera_thread.cap.read()
        if ret:
            frame = frame[:,80:560,:]
            img_path = "./session/"+self.session_id+"/captured_frame"+str(self.count)+".jpg"
            cv2.imwrite(img_path, frame)
            self.image_captured.emit(img_path)
        
        # èªéŸ³è½‰æ–‡å­—
        segments, _ = whisper_model.transcribe(audio_file)
        transcribed_text = " ".join([segment.text for segment in segments])
        self.transcription_done.emit(transcribed_text)

class AIProcessingThread(QThread):
    response_ready = pyqtSignal(bool)
    response_streaming = pyqtSignal(str)
    full_response = pyqtSignal(str)
    
    def __init__(self, chat_history, user_text, img_path=None):
        super().__init__()
        self.chat_history = chat_history
        self.user_text = user_text
        self.img_path = img_path
    
    def run(self):
        messages = list(self.chat_history)
        
        if self.img_path:
            base64_image = encode_image(self.img_path)
            messages.append({"role": "user", "content": [
                        {
                            "type": "text",
                            "text": "(è«‹æ ¹æ“šåœ–ç‰‡å›ç­”)"+self.user_text
                        },
                        {"type": "image_url", "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }}]
                    })
        else:
            messages.append({"role": "user", "content": self.user_text})
        
        try:
            response = openai.chat.completions.create(
                model=GPT_MODEL_NAME,
                messages=messages,
                stream=True,
            )
            self.response_ready.emit(True)
            full_ai_text = ""
            for chunk in response:
                if chunk.choices[0].delta.content:
                    partial_text = chunk.choices[0].delta.content
                    full_ai_text += partial_text
                    self.response_streaming.emit(partial_text)  # é€æ­¥é¡¯ç¤º
        except:
            full_ai_text = ""
        finally:
            self.full_response.emit(full_ai_text)

        # if "çš„åœ–ç‰‡" in ai_text and "åƒè€ƒ" in ai_text:
        #     ai_text = 0
        #     image_response = openai.Image.create(
        #         #model=DALL_E_MODEL,
        #         prompt=ai_text,
        #         n=1,
        #         size="256x256"
        #     )
        #     image_url = image_response["data"][0]["url"]
        #     self.image_generated.emit(image_url)

class AICoachApp(QWidget):
    def __init__(self):
        super().__init__()
        self.initUI()
        self.initCamera()
        self.img_path = None
        self.captured_frame_count = 0
        self.session_id = "0"
        self.load_session()
    
    def initUI(self):
        self.setWindowTitle("AI å¥èº«æ•™ç·´")
        self.setGeometry(100, 100, 1200, 600)
        
        # å·¦å´ - å°è©±ç´€éŒ„èˆ‡è¼¸å…¥
        self.conversation = QTextEdit(self)
        self.conversation.setReadOnly(True)
        
        self.text_input = QTextEdit(self)
        self.text_input.setPlaceholderText("è¼¸å…¥ä½ çš„å•é¡Œ...")

        self.user_prefix = '[ğŸ§‘ ä½ ]: '
        self.ai_prefix = '[ğŸ¤– AI æ•™ç·´]: '

        # è¼‰å…¥å°è©±
        # æ–°å¢ session é¸æ“‡ä¸‹æ‹‰å¼é¸å–® (0~10)
        self.session_selector = QComboBox(self)
        self.session_selector.addItems([str(i) for i in range(11)])  # 0~10
        self.session_selector.currentIndexChanged.connect(self.change_session)
        session_layout = QHBoxLayout()
        session_layout.addWidget(QLabel("é¸æ“‡å°è©±ç´€éŒ„:"))
        session_layout.addWidget(self.session_selector)

        self.clear_button = QPushButton("ğŸ—‘ï¸ æ¸…é™¤å°è©±", self)
        self.clear_button.clicked.connect(self.clear_session)
        self.save_button = QPushButton("ğŸ’¾ å„²å­˜å°è©±", self)
        self.save_button.clicked.connect(self.save_session)
        self.load_button = QPushButton("ğŸ“‚ è¼‰å…¥å°è©±", self)
        self.load_button.clicked.connect(self.load_session)
        conversation = QHBoxLayout()
        conversation.addWidget(self.clear_button)
        conversation.addWidget(self.save_button)
        conversation.addWidget(self.load_button)
        
        self.send_button = QPushButton("ç™¼é€", self)
        self.send_button.clicked.connect(self.process_text_input)
        
        self.voice_button = QPushButton("ğŸ¤ èªéŸ³è¼¸å…¥", self)
        self.voice_button.clicked.connect(self.start_voice_processing)
        
        left_layout = QVBoxLayout()
        left_layout.addLayout(session_layout)
        left_layout.addLayout(conversation)
        left_layout.addWidget(QLabel("å°è©±ç´€éŒ„:"))
        left_layout.addWidget(self.conversation, 5)
        left_layout.addWidget(QLabel("è¼¸å…¥:"))
        left_layout.addWidget(self.text_input, 1)
        left_layout.addWidget(self.send_button)
        left_layout.addWidget(self.voice_button)
        
        # å³å´ - æ”å½±æ©Ÿç•«é¢
        self.camera_label = QLabel(self)
        self.camera_label.setSizePolicy(QSizePolicy.Policy.Expanding, QSizePolicy.Policy.Expanding)  # è®“ QLabel å¯èª¿æ•´å¤§å°
        self.camera_label.setMinimumSize(300, 300)  # è¨­å®šæœ€å°å¤§å°ï¼Œé¿å…éå°
        
        right_layout = QVBoxLayout()
        right_layout.addWidget(QLabel("å³æ™‚æ”å½±æ©Ÿç•«é¢:"))
        right_layout.addWidget(self.camera_label)
        
        # ä¸»ä½ˆå±€
        main_layout = QHBoxLayout()
        main_layout.addLayout(left_layout, 3)
        main_layout.addLayout(right_layout, 2)
        
        self.setLayout(main_layout)
        
    def process_text_input(self):
        self.voice_button.setDisabled(True)
        self.send_button.setDisabled(True)
        user_text = self.text_input.toPlainText()
        if user_text.strip():
            self.conversation.append(self.user_prefix+f'{user_text}')
            self.text_input.clear()
            self.img_path = None
        self.process_ai_response(user_text)
        self.chat_history.append({"role": "user", "content": user_text})
    
    def initCamera(self):
        self.camera_thread = CameraThread()
        self.camera_thread.frame_update.connect(self.update_frame)
        self.camera_thread.start()
    
    def update_frame(self, qimg):
        if not self.camera_label.size().isEmpty():
            qpixmap = QPixmap.fromImage(qimg)
            qpixmap = qpixmap.scaled(self.camera_label.size(), Qt.AspectRatioMode.KeepAspectRatio, Qt.TransformationMode.SmoothTransformation)
            self.camera_label.setPixmap(qpixmap)
    
    def start_voice_processing(self):
        self.voice_button.setDisabled(True)
        self.send_button.setDisabled(True)
        self.voice_thread = VoiceProcessingThread(self.camera_thread, self.session_id, self.captured_frame_count)
        self.voice_thread.transcription_done.connect(self.display_transcription)
        self.voice_thread.image_captured.connect(self.display_image)
        self.voice_thread.start()
        self.voice_button.setText("ğŸ™ï¸ éŒ„éŸ³ä¸­...")
    
    def display_transcription(self, text):
        self.conversation.append(text)
        if text == "âŒ éŒ„éŸ³è¶…æ™‚ï¼Œè«‹å†è©¦ä¸€æ¬¡":
            self.recover_voice_button()
            self.recover_send_button()
            return    
        self.process_ai_response(text)
        self.chat_history.append({"role": "user", "content": text})
    
    def process_ai_response(self, user_text):
        if user_text == "":
            self.recover_voice_button()
            self.recover_send_button()
            return
        self.captured_frame_count += 1
        self.voice_button.setText("ğŸ¤– å›æ‡‰ä¸­...")
        self.ai_thread = AIProcessingThread(self.chat_history, user_text, self.img_path)
        self.ai_thread.response_ready.connect(self.display_ai_response_ready)
        self.ai_thread.response_streaming.connect(self.display_ai_response)
        self.ai_thread.full_response.connect(self.display_ai_response_end)
        self.ai_thread.start()

    def display_ai_response_ready(self):
        self.conversation.append(self.ai_prefix)
    
    def display_ai_response(self, ai_text):
        cursor = self.conversation.textCursor()
        cursor.movePosition(cursor.MoveOperation.End)
        cursor.insertText(ai_text)
        self.conversation.setTextCursor(cursor)
        self.conversation.ensureCursorVisible()

    def display_ai_response_end(self, full_ai_text):
        if full_ai_text == "":
            self.conversation.append(self.ai_prefix+'âŒ AI å›æ‡‰å¤±æ•—ï¼Œè«‹é‡è©¦ã€‚')
        else:
            self.chat_history.append({"role": "assistant", "content": full_ai_text})
        self.recover_voice_button()
        self.recover_send_button()
        self.img_path = None

    def recover_voice_button(self):
        self.voice_button.setDisabled(False)
        self.voice_button.setText("ğŸ¤ èªéŸ³è¼¸å…¥")

    def recover_send_button(self):
        self.send_button.setDisabled(False)
    
    def display_image(self, img_path):
        self.img_path = img_path

        # è®€å–åœ–ç‰‡ä¸¦è½‰æ›ç‚º QPixmap
        pixmap = QPixmap(img_path)
        if pixmap.isNull():
            print("âŒ ç„¡æ³•è®€å–åœ–ç‰‡")
            return

        # å»ºç«‹ QLabel ä¾†é¡¯ç¤ºåœ–ç‰‡
        image_label = QLabel()
        image_label.setPixmap(pixmap.scaled(400, 400, Qt.AspectRatioMode.KeepAspectRatio))

        # æ’å…¥ QLabel åˆ°å°è©±ç´€éŒ„
        self.conversation.append(self.user_prefix+'\n')
        self.conversation.insertHtml(f'<img src="{img_path}" width="300">')
    
    def closeEvent(self, event):
        self.camera_thread.stop()
        self.save_session()
        event.accept()

    def save_session(self):
        session_id = int(self.session_id)
        with open("./session/"+str(session_id)+"/chat_history.json", "w", encoding="utf-8") as fp:
            json.dump(self.chat_history, fp, indent=2, ensure_ascii=False)

        file_path = f"./session/{session_id}/conversation.html"

        with open(file_path, "w", encoding="utf-8") as file:
            file.write(self.conversation.toHtml())  # å„²å­˜å®Œæ•´ HTML æ ¼å¼
        
        #print(f"âœ… å°è©±ç´€éŒ„å·²å„²å­˜è‡³ {file_path}")

    def load_session(self):
        self.session_id = str(self.session_selector.currentText())
        
        file_path = f"./session/{self.session_id}/conversation.html"
        if os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as file:
                self.conversation.setHtml(file.read())
            with open("./session/"+self.session_id+"/chat_history.json", "r", encoding="utf-8") as fp:
                self.chat_history = json.load(fp)
            #print(f"ğŸ“‚ å·²è¼‰å…¥å°è©±ç´€éŒ„: {file_path}")
        else:
            self.clear_session()
            #print(f"âš ï¸ æ²’æœ‰æ‰¾åˆ°å°è©±ç´€éŒ„ï¼Œå»ºç«‹æ–°çš„ session {self.session_id}")

    def clear_session(self):
        self.session_id = str(self.session_selector.currentText())
        self.conversation.clear()
        self.chat_history = list(INIT_CHAT_HISTORY)
        self.count = 0
        
    def change_session(self):
        self.save_session()
        self.load_session()

        

if __name__ == '__main__':
    app = QApplication(sys.argv)
    window = AICoachApp()
    window.show()
    sys.exit(app.exec())
